{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83cd5404-4fef-4dc7-b8a4-e24a98e53d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== Imports & Hyperparameters ===============\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import medmnist\n",
    "from medmnist import INFO\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ----- Training configuration -----\n",
    "DATASET_NAME = \"pathmnist\"      # we focus on PathMNIST\n",
    "BATCH_SIZE = 256                # larger batch for better GPU utilization\n",
    "EPOCHS_CNN = 8                  # epochs for SimpleCNN\n",
    "EPOCHS_RESNET = 12              # epochs for ResNet18 (larger net)\n",
    "LR_CNN = 1e-3                   # learning rate for SimpleCNN\n",
    "LR_RESNET = 1e-4                # smaller LR for fine-tuning pretrained ResNet18\n",
    "USE_PRETRAINED_RESNET = True    # use ImageNet weights\n",
    "\n",
    "# ImageNet normalization (standard for ResNet-like models)\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "# ====================== Utils ======================\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"Return GPU device if available, otherwise CPU.\"\"\"\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set random seed for reproducibility.\"\"\"\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed) \n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def denormalize_image(img_tensor):\n",
    "    \"\"\"\n",
    "    Undo ImageNet normalization for visualization.\n",
    "    img_tensor: tensor [3,H,W], normalized by IMAGENET_MEAN/STD.\n",
    "    Returns a tensor in [0,1].\n",
    "    \"\"\"\n",
    "    mean = torch.tensor(IMAGENET_MEAN).view(3, 1, 1).to(img_tensor.device)\n",
    "    std = torch.tensor(IMAGENET_STD).view(3, 1, 1).to(img_tensor.device)\n",
    "    img = img_tensor * std + mean\n",
    "    img = torch.clamp(img, 0.0, 1.0)\n",
    "    return img\n",
    "\n",
    "\n",
    "# ====================== Data ======================\n",
    "\n",
    "def get_medmnist_dataloaders(dataset_name=DATASET_NAME, batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Load MedMNIST dataset (train/val/test) with preprocessing.\n",
    "\n",
    "    Preprocessing:\n",
    "      - resize to 64x64 (slightly larger than original 28x28)\n",
    "      - convert to tensor in [0,1]\n",
    "      - convert 1-channel images to 3-channel\n",
    "      - apply ImageNet normalization\n",
    "\n",
    "    Train loader uses data augmentation:\n",
    "      - random horizontal flip\n",
    "      - small random rotation\n",
    "    \"\"\"\n",
    "    info = INFO[dataset_name]\n",
    "    DataClass = getattr(medmnist, info[\"python_class\"])\n",
    "    num_classes = len(info[\"label\"])\n",
    "\n",
    "    # Data augmentation for training\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.shape[0] == 1 else x),\n",
    "        transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "    ])\n",
    "\n",
    "    # Evaluation transform (no augmentation)\n",
    "    transform_eval = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.shape[0] == 1 else x),\n",
    "        transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "    ])\n",
    "\n",
    "    train_dataset = DataClass(split=\"train\", transform=transform_train, download=True)\n",
    "    val_dataset = DataClass(split=\"val\", transform=transform_eval, download=True)\n",
    "    test_dataset = DataClass(split=\"test\", transform=transform_eval, download=True)\n",
    "\n",
    "    # num_workers=0 to avoid multiprocessing issues in Jupyter on Windows\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, num_classes\n",
    "\n",
    "\n",
    "# ====================== Models ======================\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A convolutional neural network used as a baseline.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # Block 1\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        # Block 2\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        # Block 3\n",
    "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        # Block 4\n",
    "        self.conv4 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # Input 64x64 -> 32 -> 16 -> 8 -> 4 after 4 poolings\n",
    "        self.fc1 = nn.Linear(256 * 4 * 4, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # [B,64,32,32]\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))  # [B,128,16,16]\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))  # [B,256,8,8]\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))  # [B,256,4,4]\n",
    "        x = x.view(x.size(0), -1)                       # [B,256*4*4]\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_resnet18_model(num_classes, use_pretrained=True):\n",
    "    \"\"\"\n",
    "    Create a ResNet18 model with modified final layer.\n",
    "    Used as a stronger model compared to SimpleCNN.\n",
    "    \"\"\"\n",
    "    if use_pretrained:\n",
    "        weights = ResNet18_Weights.DEFAULT\n",
    "        model = resnet18(weights=weights)\n",
    "    else:\n",
    "        model = resnet18(weights=None)\n",
    "\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "# ====================== Training & Evaluation ======================\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    \"\"\"Train model for one epoch and return average loss.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, targets in dataloader:\n",
    "        images = images.to(device)\n",
    "\n",
    "        # Targets from MedMNIST can be tensors or numpy arrays\n",
    "        if not isinstance(targets, torch.Tensor):\n",
    "            targets = torch.tensor(targets)\n",
    "        targets = targets.squeeze()\n",
    "        if targets.ndim > 1:\n",
    "            # Multi-hot -> single label via argmax\n",
    "            targets = targets.argmax(dim=1)\n",
    "        targets = targets.long().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    return running_loss / len(dataloader.dataset)\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    \"\"\"Evaluate model accuracy.\"\"\"\n",
    "    model.eval()\n",
    "    preds_all = []\n",
    "    labels_all = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in dataloader:\n",
    "            images = images.to(device)\n",
    "\n",
    "            if not isinstance(targets, torch.Tensor):\n",
    "                targets = torch.tensor(targets)\n",
    "            targets = targets.squeeze()\n",
    "            if targets.ndim > 1:\n",
    "                targets = targets.argmax(dim=1)\n",
    "            targets = targets.long()\n",
    "\n",
    "            outputs = model(images)\n",
    "            preds = outputs.argmax(dim=1).cpu()\n",
    "\n",
    "            preds_all.extend(preds.tolist())\n",
    "            labels_all.extend(targets.tolist())\n",
    "\n",
    "    acc = accuracy_score(labels_all, preds_all)\n",
    "    return acc\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, num_epochs=5, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Train model for several epochs and keep the best one on validation set.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_acc = 0.0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_acc = evaluate(model, val_loader, device)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_state = model.state_dict()\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    print(\"Best validation accuracy:\", best_acc)\n",
    "    return model\n",
    "\n",
    "\n",
    "# ====================== Grad-CAM & Grad-CAM++ ======================\n",
    "\n",
    "class GradCAM:\n",
    "    \"\"\"Implementation of Grad-CAM.\"\"\"\n",
    "\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "\n",
    "        self.forward_hook = target_layer.register_forward_hook(self.save_activation)\n",
    "        self.backward_hook = target_layer.register_backward_hook(self.save_gradient)\n",
    "\n",
    "    def save_activation(self, module, inp, out):\n",
    "        self.activations = out.detach()\n",
    "\n",
    "    def save_gradient(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0].detach()\n",
    "\n",
    "    def generate(self, image, target_class, device):\n",
    "        \"\"\"\n",
    "        Generate Grad-CAM heatmap for a single image.\n",
    "        image: tensor [3,H,W], target_class: int\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        image = image.unsqueeze(0).to(device)\n",
    "        output = self.model(image)\n",
    "        score = output[0, target_class]\n",
    "        score.backward()\n",
    "\n",
    "        gradients = self.gradients[0]     # [C,H,W]\n",
    "        activations = self.activations[0] # [C,H,W]\n",
    "        weights = gradients.mean(dim=(1, 2))  # [C]\n",
    "\n",
    "        cam = torch.zeros(activations.shape[1:], device=device)\n",
    "        for i, w in enumerate(weights):\n",
    "            cam += w * activations[i]\n",
    "\n",
    "        cam = F.relu(cam)\n",
    "        cam = cam - cam.min()\n",
    "        cam = cam / (cam.max() + 1e-6)\n",
    "        return cam.cpu().numpy()\n",
    "\n",
    "    def close(self):\n",
    "        self.forward_hook.remove()\n",
    "        self.backward_hook.remove()\n",
    "\n",
    "\n",
    "class GradCAMPlusPlus(GradCAM):\n",
    "    \"\"\"Implementation of Grad-CAM++ using higher-order gradients.\"\"\"\n",
    "\n",
    "    def generate(self, image, target_class, device):\n",
    "        self.model.eval()\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        image = image.unsqueeze(0).to(device)\n",
    "        output = self.model(image)\n",
    "        score = output[0, target_class]\n",
    "        score.backward(retain_graph=True)\n",
    "\n",
    "        gradients = self.gradients[0]     # [C,H,W]\n",
    "        activations = self.activations[0] # [C,H,W]\n",
    "\n",
    "        grad2 = gradients ** 2\n",
    "        grad3 = gradients ** 3\n",
    "        sum_act = (activations * grad3).sum(dim=(1, 2), keepdim=True)\n",
    "\n",
    "        alpha = grad2 / (2.0 * grad2 + sum_act + 1e-6)\n",
    "        weights = (alpha * F.relu(gradients)).sum(dim=(1, 2))  # [C]\n",
    "\n",
    "        cam = torch.zeros(activations.shape[1:], device=device)\n",
    "        for i, w in enumerate(weights):\n",
    "            cam += w * activations[i]\n",
    "\n",
    "        cam = F.relu(cam)\n",
    "        cam = cam - cam.min()\n",
    "        cam = cam / (cam.max() + 1e-6)\n",
    "        return cam.cpu().numpy()\n",
    "\n",
    "\n",
    "# ====================== Visualization ======================\n",
    "\n",
    "def overlay_heatmap(image, heatmap, alpha=0.4):\n",
    "    \"\"\"\n",
    "    Overlay heatmap over the de-normalized image.\n",
    "    image: tensor [3,H,W] (normalized)\n",
    "    heatmap: numpy array [h,w] in [0,1] (may be smaller than image)\n",
    "    \"\"\"\n",
    "    # de-normalize image for display\n",
    "    img = denormalize_image(image).permute(1, 2, 0).cpu().numpy()  # [H,W,3]\n",
    "    H, W = img.shape[:2]\n",
    "\n",
    "    # upsample heatmap to match image size using bilinear interpolation\n",
    "    heatmap_t = torch.tensor(heatmap, dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # [1,1,h,w]\n",
    "    heatmap_up = F.interpolate(heatmap_t, size=(H, W), mode=\"bilinear\", align_corners=False)\n",
    "    heatmap_up = heatmap_up.squeeze().cpu().numpy()  # [H,W]\n",
    "\n",
    "    # convert heatmap to RGB\n",
    "    heatmap_rgb = plt.cm.jet(heatmap_up)[..., :3]  # [H,W,3]\n",
    "\n",
    "    # blend image and heatmap\n",
    "    blended = (1 - alpha) * img + alpha * heatmap_rgb\n",
    "    blended = np.clip(blended, 0.0, 1.0)\n",
    "    return blended\n",
    "\n",
    "\n",
    "def save_examples(model, gradcam, gradcampp, test_loader, device, outdir=\"xai_outputs\"):\n",
    "    \"\"\"\n",
    "    Save a few example images with Grad-CAM and Grad-CAM++ overlays.\n",
    "    \"\"\"\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    images, labels = next(iter(test_loader))\n",
    "    images = images[:5]\n",
    "    labels = labels.squeeze()\n",
    "    if labels.ndim > 1:\n",
    "        labels = labels.argmax(dim=1)\n",
    "    labels = labels.long()\n",
    "\n",
    "    for i in range(len(images)):\n",
    "        img = images[i]\n",
    "        cls = int(labels[i].item())\n",
    "\n",
    "        cam = gradcam.generate(img, cls, device)\n",
    "        campp = gradcampp.generate(img, cls, device)\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(9, 3))\n",
    "\n",
    "        # Original (de-normalized)\n",
    "        axes[0].imshow(denormalize_image(img).permute(1, 2, 0).cpu().numpy())\n",
    "        axes[0].set_title(f\"Original (class {cls})\")\n",
    "        axes[0].axis(\"off\")\n",
    "\n",
    "        axes[1].imshow(overlay_heatmap(img, cam))\n",
    "        axes[1].set_title(\"Grad-CAM\")\n",
    "        axes[1].axis(\"off\")\n",
    "\n",
    "        axes[2].imshow(overlay_heatmap(img, campp))\n",
    "        axes[2].set_title(\"Grad-CAM++\")\n",
    "        axes[2].axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        fig.savefig(os.path.join(outdir, f\"example_{i}.png\"))\n",
    "        plt.close(fig)\n",
    "\n",
    "    print(f\"Saved XAI examples to: {outdir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5e2abf7-8c98-4021-8fa8-8949193be495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 206M/206M [00:09<00:00, 21.6MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: pathmnist, num_classes: 9\n",
      "\n",
      "Training SimpleCNN (baseline)...\n",
      "Epoch 1/8 | Train Loss: 0.7970 | Val Acc: 0.7649\n",
      "Epoch 2/8 | Train Loss: 0.4169 | Val Acc: 0.8780\n",
      "Epoch 3/8 | Train Loss: 0.2951 | Val Acc: 0.9252\n",
      "Epoch 4/8 | Train Loss: 0.2376 | Val Acc: 0.8973\n",
      "Epoch 5/8 | Train Loss: 0.2071 | Val Acc: 0.9318\n",
      "Epoch 6/8 | Train Loss: 0.1811 | Val Acc: 0.9441\n",
      "Epoch 7/8 | Train Loss: 0.1629 | Val Acc: 0.9241\n",
      "Epoch 8/8 | Train Loss: 0.1488 | Val Acc: 0.9298\n",
      "Best validation accuracy: 0.9441223510595762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\dashabi/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN Test Accuracy: 0.8328690807799443\n",
      "\n",
      "Training ResNet18 (improved model)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 58.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12 | Train Loss: 0.3361 | Val Acc: 0.9507\n",
      "Epoch 2/12 | Train Loss: 0.1459 | Val Acc: 0.9646\n",
      "Epoch 3/12 | Train Loss: 0.1026 | Val Acc: 0.9706\n",
      "Epoch 4/12 | Train Loss: 0.0795 | Val Acc: 0.9757\n",
      "Epoch 5/12 | Train Loss: 0.0601 | Val Acc: 0.9722\n",
      "Epoch 6/12 | Train Loss: 0.0499 | Val Acc: 0.9778\n",
      "Epoch 7/12 | Train Loss: 0.0403 | Val Acc: 0.9748\n",
      "Epoch 8/12 | Train Loss: 0.0357 | Val Acc: 0.9690\n",
      "Epoch 9/12 | Train Loss: 0.0301 | Val Acc: 0.9837\n",
      "Epoch 10/12 | Train Loss: 0.0257 | Val Acc: 0.9806\n",
      "Epoch 11/12 | Train Loss: 0.0241 | Val Acc: 0.9822\n",
      "Epoch 12/12 | Train Loss: 0.0232 | Val Acc: 0.9810\n",
      "Best validation accuracy: 0.9837065173930428\n",
      "ResNet18 Test Accuracy: 0.9018105849582173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dashabi\\OneDrive\\Desktop\\ECE1513\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved XAI examples to: xai_outputs\n",
      "\n",
      "Done. Models saved in 'checkpoints', XAI images in 'xai_outputs'.\n"
     ]
    }
   ],
   "source": [
    "# ================== Training & XAI Pipeline ==================\n",
    "\n",
    "set_seed(42)\n",
    "device = get_device()\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ----- Load data -----\n",
    "train_loader, val_loader, test_loader, num_classes = \\\n",
    "    get_medmnist_dataloaders(DATASET_NAME, BATCH_SIZE)\n",
    "\n",
    "print(f\"Dataset: {DATASET_NAME}, num_classes: {num_classes}\")\n",
    "\n",
    "# ----- Train SimpleCNN (baseline) -----\n",
    "print(\"\\nTraining SimpleCNN (baseline)...\")\n",
    "simple_cnn = SimpleCNN(num_classes)\n",
    "simple_cnn = train_model(simple_cnn, train_loader, val_loader, device,\n",
    "                         num_epochs=EPOCHS_CNN, lr=LR_CNN)\n",
    "test_acc_cnn = evaluate(simple_cnn, test_loader, device)\n",
    "print(\"SimpleCNN Test Accuracy:\", test_acc_cnn)\n",
    "\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "torch.save(simple_cnn.state_dict(), os.path.join(\"checkpoints\", \"simple_cnn.pth\"))\n",
    "\n",
    "# ----- Train ResNet18 (improved, larger model) -----\n",
    "print(\"\\nTraining ResNet18 (improved model)...\")\n",
    "resnet18_model = get_resnet18_model(num_classes, use_pretrained=USE_PRETRAINED_RESNET)\n",
    "resnet18_model = train_model(resnet18_model, train_loader, val_loader, device,\n",
    "                             num_epochs=EPOCHS_RESNET, lr=LR_RESNET)\n",
    "test_acc_resnet18 = evaluate(resnet18_model, test_loader, device)\n",
    "print(\"ResNet18 Test Accuracy:\", test_acc_resnet18)\n",
    "\n",
    "torch.save(resnet18_model.state_dict(), os.path.join(\"checkpoints\", \"resnet18.pth\"))\n",
    "\n",
    "# ----- Use ResNet18 for Grad-CAM / Grad-CAM++ -----\n",
    "final_model = resnet18_model\n",
    "target_layer = final_model.layer4[-1].conv2  # last conv layer of ResNet18\n",
    "\n",
    "gradcam = GradCAM(final_model, target_layer)\n",
    "gradcampp = GradCAMPlusPlus(final_model, target_layer)\n",
    "\n",
    "save_examples(final_model, gradcam, gradcampp, test_loader, device,\n",
    "              outdir=\"xai_outputs\")\n",
    "\n",
    "gradcam.close()\n",
    "gradcampp.close()\n",
    "\n",
    "print(\"\\nDone. Models saved in 'checkpoints', XAI images in 'xai_outputs'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8da4bf-ab80-4d06-bf7d-a728b2ebf2b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
